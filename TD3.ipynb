{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnPteJq0FEs4rNNCcJXzxE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArshT/Reinforcement_Learning_Basic/blob/master/TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMt8qZJg-zbu",
        "outputId": "1b993374-535c-4647-feb8-d53882d43005"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "import gym\n",
        "env = gym.make(\"LunarLander-v2\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 17.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 17.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 10.5MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 5.2MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dXin3qrbG6m"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as Optim\n",
        "import numpy as np\n",
        "from torch.distributions import MultivariateNormal\n",
        "\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self,input_dims,fc1_dims,fc2_dims,action_dims,device):\n",
        "    super(Actor,self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(input_dims,fc1_dims)\n",
        "    f1 = 1 / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "    torch.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
        "    torch.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
        "    self.bn1 = nn.LayerNorm(fc1_dims)\n",
        "\n",
        "    self.fc2 = nn.Linear(fc1_dims,fc2_dims)\n",
        "    f2 = 1 / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "    torch.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
        "    torch.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
        "    self.bn2 = nn.LayerNorm(fc2_dims)\n",
        "\n",
        "    f3 = 0.003\n",
        "    self.mu = nn.Linear(fc2_dims,action_dims)\n",
        "    torch.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
        "    torch.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
        "\n",
        "    self.device = device\n",
        "    self.to(self.device)\n",
        "  \n",
        "  def forward(self,state):\n",
        "\n",
        "    x = F.relu(self.bn1(self.fc1(state)))\n",
        "    x = F.relu(self.bn2(self.fc2(x)))\n",
        "\n",
        "    action_mu = torch.tanh(self.mu(x))\n",
        "\n",
        "    return action_mu\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self,input_dims,fc1_dims,fc2_dims,action_dims,device):\n",
        "    super(Critic,self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(input_dims,fc1_dims)\n",
        "    f1 = 1 / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "    torch.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
        "    torch.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
        "    self.bn1 = nn.LayerNorm(fc1_dims)\n",
        "\n",
        "    self.fc2 = nn.Linear(fc1_dims,fc2_dims)\n",
        "    f2 = 1 / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "    torch.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
        "    torch.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
        "    self.bn2 = nn.LayerNorm(fc2_dims)\n",
        "\n",
        "    self.action_value_layer = nn.Linear(action_dims,fc2_dims)\n",
        "\n",
        "    f3 = 0.003\n",
        "    self.q = nn.Linear(fc2_dims,1)\n",
        "    torch.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
        "    torch.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
        "\n",
        "    self.device = device\n",
        "    self.to(self.device)\n",
        "  \n",
        "  def forward(self,state,action):\n",
        "\n",
        "    state_value = F.relu(self.bn1(self.fc1(state)))\n",
        "    state_value = self.bn2(self.fc2(state_value))\n",
        "\n",
        "    action_value = F.relu(self.action_value_layer(action))\n",
        "\n",
        "    state_action_value = F.relu(torch.add(state_value,action_value))\n",
        "    state_action_value = self.q(state_action_value)\n",
        "\n",
        "    return state_action_value\n",
        "  \n",
        "\n",
        "class Agent:\n",
        "  def __init__(self,alpha,beta,input_dims,fc1_dims,fc2_dims,action_dims,tau,env,action_std_decay_rate,device,\n",
        "                 min_action_std, gamma=0.99,max_size=1000000,batch_size=64,action_std_init=0.6,policy_delay=2,noise_clip=0.5):\n",
        "    \n",
        "    self.action_dims = action_dims\n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.env = env\n",
        "    self.batch_size = batch_size\n",
        "    self.device = device\n",
        "\n",
        "    self.action_dims=action_dims\n",
        "    self.action_std_decay_rate = action_std_decay_rate\n",
        "    self.min_action_std = min_action_std\n",
        "\n",
        "    self.actor = Actor(input_dims=input_dims,fc1_dims=fc1_dims,fc2_dims=fc2_dims,action_dims=action_dims,device=device)\n",
        "    self.target_actor = Actor(input_dims=input_dims,fc1_dims=fc1_dims,fc2_dims=fc2_dims,action_dims=action_dims,device=device)\n",
        "\n",
        "    self.critic_1 = Critic(input_dims=input_dims,fc1_dims=fc1_dims,fc2_dims=fc2_dims,action_dims=action_dims,device=device)\n",
        "    self.target_critic_1 = Critic(input_dims=input_dims,fc1_dims=fc1_dims,fc2_dims=fc2_dims,action_dims=action_dims,device=device)\n",
        "    \n",
        "    self.critic_2 = Critic(input_dims=input_dims,fc1_dims=fc1_dims,fc2_dims=fc2_dims,action_dims=action_dims,device=device)\n",
        "    self.target_critic_2 = Critic(input_dims=input_dims,fc1_dims=fc1_dims,fc2_dims=fc2_dims,action_dims=action_dims,device=device)\n",
        "\n",
        "    self.critic_optimizer_1 = Optim.Adam(self.critic_1.parameters(),lr=beta)\n",
        "    self.critic_optimizer_2 = Optim.Adam(self.critic_2.parameters(),lr=beta)\n",
        "    self.actor_optimizer = Optim.Adam(self.actor.parameters(),lr=alpha)\n",
        "\n",
        "    self.action_std = action_std_init\n",
        "    self.action_var = torch.full((self.action_dims,), self.action_std * self.action_std).to(device)\n",
        "\n",
        "    self.policy_delay = policy_delay\n",
        "    self.noise_clip = noise_clip\n",
        "\n",
        "    self.mem_size = max_size\n",
        "    self.mem_cntr = 0\n",
        "    self.state_memory = np.zeros((self.mem_size, input_dims))\n",
        "    self.new_state_memory = np.zeros((self.mem_size, input_dims))\n",
        "    self.action_memory = np.zeros((self.mem_size,action_dims))\n",
        "    self.reward_memory = np.zeros((self.mem_size,1))\n",
        "    self.terminal_memory = np.zeros((self.mem_size,1), dtype=np.float32)\n",
        "\n",
        "    self.update_network_parameters(tau=1)\n",
        "\n",
        "  \n",
        "  def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        actor_params = self.actor.named_parameters()\n",
        "        target_actor_params = self.target_actor.named_parameters()\n",
        "        critic_params_1 = self.critic_1.named_parameters()\n",
        "        target_critic_params_1 = self.target_critic_1.named_parameters()\n",
        "        critic_params_2 = self.critic_2.named_parameters()\n",
        "        target_critic_params_2 = self.target_critic_2.named_parameters()\n",
        "\n",
        "\n",
        "        actor_state_dict = dict(actor_params)\n",
        "        target_actor_dict = dict(target_actor_params)\n",
        "        critic_state_dict_1 = dict(critic_params_1)\n",
        "        target_critic_dict_1 = dict(target_critic_params_1)\n",
        "        critic_state_dict_2 = dict(critic_params_2)\n",
        "        target_critic_dict_2 = dict(target_critic_params_2)\n",
        "\n",
        "        for name in critic_state_dict_1:\n",
        "            critic_state_dict_1[name] = tau*critic_state_dict_1[name].clone() + \\\n",
        "                                      (1-tau)*target_critic_dict_1[name].clone()\n",
        "\n",
        "        self.target_critic_1.load_state_dict(critic_state_dict_1)\n",
        "\n",
        "\n",
        "        for name in critic_state_dict_2:\n",
        "            critic_state_dict_2[name] = tau*critic_state_dict_2[name].clone() + \\\n",
        "                                      (1-tau)*target_critic_dict_2[name].clone()\n",
        "\n",
        "        self.target_critic_2.load_state_dict(critic_state_dict_2)\n",
        "\n",
        "\n",
        "        for name in actor_state_dict:\n",
        "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
        "                                      (1-tau)*target_actor_dict[name].clone()\n",
        "        self.target_actor.load_state_dict(actor_state_dict)\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  def set_action_std(self, new_action_std):\n",
        "    self.action_std = new_action_std\n",
        "    self.action_var = torch.full((self.action_dims,), self.action_std * self.action_std).to(self.device)\n",
        "\n",
        "  \n",
        "  \n",
        "  def decay_action_std(self):\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    \n",
        "    self.action_std = self.action_std - self.action_std_decay_rate\n",
        "    self.action_std = round(self.action_std, 4)\n",
        "    if (self.action_std <= self.min_action_std):\n",
        "      self.action_std = self.min_action_std\n",
        "      print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "      self.set_action_std(self.action_std)\n",
        "    else:\n",
        "      print(\"setting actor output action_std to : \", self.action_std)\n",
        "      self.set_action_std(self.action_std)\n",
        "    \n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  \n",
        "  def remember(self,state,action,reward,new_state,done):\n",
        "    index = self.mem_cntr % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.new_state_memory[index] = new_state\n",
        "    self.terminal_memory[index] = float(1- done)\n",
        "    self.mem_cntr += 1\n",
        "  \n",
        "  \n",
        "  def choose_action(self,observation):\n",
        "    self.actor.eval()\n",
        "    with torch.no_grad():\n",
        "      state = torch.FloatTensor(observation).to(self.actor.device)\n",
        "      action_mean = self.actor.forward(state).to(self.actor.device)\n",
        "      cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "      dist = MultivariateNormal(action_mean, cov_mat)\n",
        "      \n",
        "    action = dist.sample()\n",
        "    low = self.env.action_space.low[0]\n",
        "    high = self.env.action_space.high[0]\n",
        "    action = torch.clamp(action,low,high)\n",
        "    \n",
        "    self.actor.train()\n",
        "      \n",
        "    return action.detach().cpu().numpy().flatten()\n",
        "  \n",
        "\n",
        "  def learn(self,n_iter):\n",
        "\n",
        "    for k in range(n_iter):\n",
        "      if self.mem_cntr < self.batch_size:\n",
        "        return\n",
        "      else:\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "      batch = np.random.choice(max_mem, self.batch_size)\n",
        "\n",
        "      states = self.state_memory[batch]\n",
        "      actions = self.action_memory[batch]\n",
        "      rewards = self.reward_memory[batch]\n",
        "      next_states = self.new_state_memory[batch]\n",
        "      terminals = self.terminal_memory[batch]\n",
        "\n",
        "      rewards = torch.tensor(rewards, dtype=torch.float).to(self.critic_1.device)\n",
        "      terminals = torch.tensor(terminals).to(self.critic_1.device)\n",
        "      next_states = torch.tensor(next_states, dtype=torch.float).to(self.critic_1.device)\n",
        "      actions = torch.tensor(actions, dtype=torch.float).to(self.critic_1.device)\n",
        "      states = torch.tensor(states, dtype=torch.float).to(self.critic_1.device)\n",
        "\n",
        "      self.target_actor.eval()\n",
        "      self.target_critic_1.eval()\n",
        "      self.critic_1.eval()\n",
        "      self.target_critic_2.eval()\n",
        "      self.critic_2.eval()\n",
        "\n",
        "      next_actions = self.target_actor.forward(next_states)\n",
        "      noise =  torch.normal(0,self.action_std, size=actions.shape).to(self.device)\n",
        "      noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "      next_actions = (next_actions + noise)\n",
        "      next_actions = torch.clamp(next_actions,self.env.action_space.low[0],self.env.action_space.high[0])\n",
        "\n",
        "      next_critic_value_1 = self.target_critic_1.forward(next_states,next_actions)\n",
        "      next_critic_value_2 = self.target_critic_2.forward(next_states,next_actions)\n",
        "      next_critic_value = torch.min(next_critic_value_1,next_critic_value_2)\n",
        "      critic_value_1 = self.critic_1.forward(states,actions)\n",
        "      critic_value_2 = self.critic_2.forward(states,actions)\n",
        "\n",
        "      targets = rewards + self.gamma*next_critic_value*terminals\n",
        "      targets = torch.tensor(targets).to(self.critic_1.device)\n",
        "      targets = targets.view(self.batch_size, 1)\n",
        "\n",
        "      self.critic_1.train()\n",
        "      self.critic_optimizer_1.zero_grad()\n",
        "      critic_loss_1 = F.mse_loss(targets, critic_value_1)\n",
        "      critic_loss_1.backward()\n",
        "      self.critic_optimizer_1.step()\n",
        "\n",
        "      self.critic_2.train()\n",
        "      self.critic_optimizer_2.zero_grad()\n",
        "      critic_loss_2 = F.mse_loss(targets, critic_value_2)\n",
        "      critic_loss_2.backward()\n",
        "      self.critic_optimizer_2.step()\n",
        "\n",
        "      if k%self.policy_delay == 0:\n",
        "        \n",
        "        self.critic_1.eval()\n",
        "        mu = self.actor.forward(states)\n",
        "        self.actor.train()\n",
        "        actor_loss = -self.critic_1.forward(states,mu)\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.mean().backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        self.update_network_parameters()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1lsP8PmbJXf",
        "outputId": "981a88e3-a3e8-4410-a9ec-34c288d806ce"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "agent = Agent(alpha=0.001, beta=0.001, input_dims=8, tau=0.001, env=env,action_std_decay_rate=0.05,gamma=0.99,\n",
        "                 min_action_std=0.05,batch_size=128, fc1_dims=400, fc2_dims=300, action_dims=2,action_std_init=0.2,device='cuda')\n",
        "\n",
        "action_std_decay_freq = int(7.5e4) \n",
        "#action_std_decay_freq = int(2000) \n",
        "\n",
        "score_history = []\n",
        "timesteps = 0\n",
        "for i in range(5000):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    t = 0\n",
        "    while not done:\n",
        "        act = agent.choose_action(obs)\n",
        "        new_state, reward, done, info = env.step(act)\n",
        "        agent.remember(obs, act, reward, new_state, int(done))\n",
        "        score += reward\n",
        "        obs = new_state\n",
        "        timesteps += 1\n",
        "        \n",
        "        if timesteps % action_std_decay_freq == 0:\n",
        "           pass\n",
        "           # agent.decay_action_std()\n",
        "            \n",
        "        if done:\n",
        "            agent.learn(t)\n",
        "            break\n",
        "        #env.render()\n",
        "        t += 1\n",
        "    score_history.append(score)\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print('episode ', i, 'score %.2f' % score,\n",
        "          'trailing 10 games avg %.3f' % np.mean(score_history[-10:]),\n",
        "          'trailing 100 games avg %.3f' % np.mean(score_history[-100:]),\n",
        "          'timesteps:',timesteps)\n",
        "    else:\n",
        "        print('episode ', i, 'score %.2f' % score,\n",
        "          'timesteps:',timesteps)\n",
        "      \n",
        "    avg_100 = np.mean(score_history[-100:])\n",
        "    if avg_100 >= 210:\n",
        "      print(\"####SOLVED!!####\")\n",
        "      break\n",
        "\n",
        "\n",
        "avg_score = 0\n",
        "print(\"\")\n",
        "for i in range(50):\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  score = 0\n",
        "  while not done:\n",
        "    act = agent.choose_action(obs)\n",
        "    new_state, reward, done, info = env.step(act)\n",
        "    obs = new_state\n",
        "    score += reward\n",
        "  avg_score += score\n",
        "  print(\"Episode Reward:\",score)\n",
        "\n",
        "print(\"\")\n",
        "print(avg_score/50)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode  0 score -49.47 trailing 10 games avg -49.474 trailing 100 games avg -49.474 timesteps: 75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:255: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode  1 score -120.41 timesteps: 216\n",
            "episode  2 score -1052.98 timesteps: 319\n",
            "episode  3 score -467.02 timesteps: 458\n",
            "episode  4 score -290.15 timesteps: 602\n",
            "episode  5 score -846.64 timesteps: 765\n",
            "episode  6 score -256.29 timesteps: 865\n",
            "episode  7 score -286.55 timesteps: 972\n",
            "episode  8 score -190.86 timesteps: 1071\n",
            "episode  9 score -186.74 timesteps: 1204\n",
            "episode  10 score -251.95 trailing 10 games avg -394.960 trailing 100 games avg -363.552 timesteps: 1386\n",
            "episode  11 score -90.29 timesteps: 1611\n",
            "episode  12 score -158.88 timesteps: 2042\n",
            "episode  13 score -164.38 timesteps: 2324\n",
            "episode  14 score -156.36 timesteps: 2517\n",
            "episode  15 score -225.59 timesteps: 2695\n",
            "episode  16 score -224.52 timesteps: 2868\n",
            "episode  17 score -266.45 timesteps: 2988\n",
            "episode  18 score -635.91 timesteps: 3469\n",
            "episode  19 score -146.29 timesteps: 3738\n",
            "episode  20 score -372.58 trailing 10 games avg -244.125 trailing 100 games avg -306.682 timesteps: 3841\n",
            "episode  21 score -442.53 timesteps: 3960\n",
            "episode  22 score -311.19 timesteps: 4118\n",
            "episode  23 score -230.78 timesteps: 4208\n",
            "episode  24 score -228.40 timesteps: 4312\n",
            "episode  25 score -338.48 timesteps: 4442\n",
            "episode  26 score -132.98 timesteps: 4681\n",
            "episode  27 score -391.76 timesteps: 4944\n",
            "episode  28 score -198.62 timesteps: 5170\n",
            "episode  29 score -361.54 timesteps: 5339\n",
            "episode  30 score -370.30 trailing 10 games avg -300.657 trailing 100 games avg -304.739 timesteps: 5507\n",
            "episode  31 score -254.61 timesteps: 5751\n",
            "episode  32 score -150.86 timesteps: 6075\n",
            "episode  33 score -180.68 timesteps: 6233\n",
            "episode  34 score -518.94 timesteps: 6424\n",
            "episode  35 score -391.08 timesteps: 6592\n",
            "episode  36 score -272.95 timesteps: 6857\n",
            "episode  37 score -153.39 timesteps: 7218\n",
            "episode  38 score -260.36 timesteps: 7498\n",
            "episode  39 score -239.66 timesteps: 7708\n",
            "episode  40 score -384.90 trailing 10 games avg -280.745 trailing 100 games avg -298.886 timesteps: 8091\n",
            "episode  41 score -627.31 timesteps: 8736\n",
            "episode  42 score -49.03 timesteps: 9097\n",
            "episode  43 score -299.99 timesteps: 9408\n",
            "episode  44 score -59.01 timesteps: 9903\n",
            "episode  45 score 128.04 timesteps: 10540\n",
            "episode  46 score 174.15 timesteps: 11175\n",
            "episode  47 score -75.30 timesteps: 11582\n",
            "episode  48 score -336.51 timesteps: 12244\n",
            "episode  49 score -115.02 timesteps: 13244\n",
            "episode  50 score -407.29 trailing 10 games avg -166.727 trailing 100 games avg -272.973 timesteps: 13941\n",
            "episode  51 score -71.26 timesteps: 14941\n",
            "episode  52 score -106.65 timesteps: 15941\n",
            "episode  53 score -67.72 timesteps: 16941\n",
            "episode  54 score -95.85 timesteps: 17941\n",
            "episode  55 score -140.16 timesteps: 18941\n",
            "episode  56 score -141.11 timesteps: 19941\n",
            "episode  57 score -75.68 timesteps: 20941\n",
            "episode  58 score -54.12 timesteps: 21941\n",
            "episode  59 score -102.64 timesteps: 22941\n",
            "episode  60 score -76.72 trailing 10 games avg -93.190 trailing 100 games avg -243.500 timesteps: 23941\n",
            "episode  61 score -141.93 timesteps: 24941\n",
            "episode  62 score -8.87 timesteps: 25941\n",
            "episode  63 score -74.49 timesteps: 26941\n",
            "episode  64 score -41.02 timesteps: 27941\n",
            "episode  65 score 92.50 timesteps: 28941\n",
            "episode  66 score 178.31 timesteps: 29890\n",
            "episode  67 score -82.40 timesteps: 30890\n",
            "episode  68 score 219.99 timesteps: 31564\n",
            "episode  69 score -79.17 timesteps: 32564\n",
            "episode  70 score -149.55 trailing 10 games avg -8.663 trailing 100 games avg -210.425 timesteps: 33564\n",
            "episode  71 score 226.01 timesteps: 34350\n",
            "episode  72 score 123.23 timesteps: 35350\n",
            "episode  73 score -4.17 timesteps: 36350\n",
            "episode  74 score -91.40 timesteps: 37350\n",
            "episode  75 score -53.43 timesteps: 38350\n",
            "episode  76 score 203.54 timesteps: 38836\n",
            "episode  77 score -91.16 timesteps: 39836\n",
            "episode  78 score -63.49 timesteps: 40836\n",
            "episode  79 score -240.51 timesteps: 41829\n",
            "episode  80 score -255.78 trailing 10 games avg -24.716 trailing 100 games avg -187.498 timesteps: 42561\n",
            "episode  81 score -69.20 timesteps: 42805\n",
            "episode  82 score -80.59 timesteps: 43805\n",
            "episode  83 score 208.21 timesteps: 44329\n",
            "episode  84 score -214.48 timesteps: 44701\n",
            "episode  85 score -198.98 timesteps: 45132\n",
            "episode  86 score -80.56 timesteps: 45376\n",
            "episode  87 score -92.03 timesteps: 46376\n",
            "episode  88 score -456.18 timesteps: 47315\n",
            "episode  89 score -52.62 timesteps: 48315\n",
            "episode  90 score -275.14 trailing 10 games avg -131.158 trailing 100 games avg -181.306 timesteps: 48589\n",
            "episode  91 score -149.93 timesteps: 48966\n",
            "episode  92 score 4.98 timesteps: 49232\n",
            "episode  93 score 184.65 timesteps: 49895\n",
            "episode  94 score -143.80 timesteps: 50150\n",
            "episode  95 score -18.70 timesteps: 50458\n",
            "episode  96 score -57.41 timesteps: 51458\n",
            "episode  97 score -176.99 timesteps: 52458\n",
            "episode  98 score -27.89 timesteps: 53458\n",
            "episode  99 score 212.38 timesteps: 53922\n",
            "episode  100 score 230.07 trailing 10 games avg 5.737 trailing 100 games avg -163.920 timesteps: 54596\n",
            "episode  101 score -27.51 timesteps: 54942\n",
            "episode  102 score 198.88 timesteps: 55473\n",
            "episode  103 score -8.40 timesteps: 56473\n",
            "episode  104 score -38.42 timesteps: 56796\n",
            "episode  105 score -39.84 timesteps: 57796\n",
            "episode  106 score 22.63 timesteps: 58796\n",
            "episode  107 score -78.90 timesteps: 59796\n",
            "episode  108 score 7.16 timesteps: 60796\n",
            "episode  109 score -79.49 timesteps: 61796\n",
            "episode  110 score -139.02 trailing 10 games avg -18.289 trailing 100 games avg -126.253 timesteps: 62796\n",
            "episode  111 score -107.38 timesteps: 63796\n",
            "episode  112 score -128.55 timesteps: 64346\n",
            "episode  113 score -90.18 timesteps: 65346\n",
            "episode  114 score -57.63 timesteps: 66346\n",
            "episode  115 score -44.06 timesteps: 67346\n",
            "episode  116 score 247.33 timesteps: 67959\n",
            "episode  117 score 187.72 timesteps: 68697\n",
            "episode  118 score -123.14 timesteps: 69697\n",
            "episode  119 score -26.50 timesteps: 70145\n",
            "episode  120 score -85.08 trailing 10 games avg -22.747 trailing 100 games avg -104.115 timesteps: 71145\n",
            "episode  121 score -96.89 timesteps: 72145\n",
            "episode  122 score -1.63 timesteps: 73145\n",
            "episode  123 score 1.98 timesteps: 74145\n",
            "episode  124 score 9.41 timesteps: 75145\n",
            "episode  125 score -65.54 timesteps: 76145\n",
            "episode  126 score 230.87 timesteps: 76778\n",
            "episode  127 score 208.59 timesteps: 77760\n",
            "episode  128 score 210.56 timesteps: 78342\n",
            "episode  129 score 29.57 timesteps: 79342\n",
            "episode  130 score -61.79 trailing 10 games avg 46.514 trailing 100 games avg -69.398 timesteps: 80342\n",
            "episode  131 score 10.92 timesteps: 81342\n",
            "episode  132 score 1.29 timesteps: 82342\n",
            "episode  133 score 197.97 timesteps: 83110\n",
            "episode  134 score 203.81 timesteps: 83867\n",
            "episode  135 score -19.58 timesteps: 84867\n",
            "episode  136 score 175.40 timesteps: 85674\n",
            "episode  137 score 69.76 timesteps: 86674\n",
            "episode  138 score 103.60 timesteps: 87674\n",
            "episode  139 score 260.08 timesteps: 88098\n",
            "episode  140 score -36.98 trailing 10 games avg 96.626 trailing 100 games avg -31.661 timesteps: 89098\n",
            "episode  141 score 244.99 timesteps: 89522\n",
            "episode  142 score 238.74 timesteps: 90029\n",
            "episode  143 score 208.67 timesteps: 90985\n",
            "episode  144 score -11.11 timesteps: 91985\n",
            "episode  145 score 240.92 timesteps: 92585\n",
            "episode  146 score 236.01 timesteps: 93493\n",
            "episode  147 score 247.15 timesteps: 94256\n",
            "episode  148 score 40.98 timesteps: 95256\n",
            "episode  149 score 200.83 timesteps: 95802\n",
            "episode  150 score 236.26 trailing 10 games avg 188.346 trailing 100 games avg 3.846 timesteps: 96694\n",
            "episode  151 score 82.84 timesteps: 97694\n",
            "episode  152 score 278.61 timesteps: 98109\n",
            "episode  153 score 242.11 timesteps: 98721\n",
            "episode  154 score 29.34 timesteps: 99721\n",
            "episode  155 score 157.72 timesteps: 100495\n",
            "episode  156 score 234.65 timesteps: 100895\n",
            "episode  157 score 237.83 timesteps: 101472\n",
            "episode  158 score 263.42 timesteps: 102109\n",
            "episode  159 score 199.05 timesteps: 102729\n",
            "episode  160 score 94.77 trailing 10 games avg 182.034 trailing 100 games avg 31.369 timesteps: 103729\n",
            "episode  161 score 263.68 timesteps: 104290\n",
            "episode  162 score 262.02 timesteps: 104530\n",
            "episode  163 score 260.46 timesteps: 104871\n",
            "episode  164 score 205.76 timesteps: 105596\n",
            "episode  165 score 260.21 timesteps: 106259\n",
            "episode  166 score 242.08 timesteps: 106746\n",
            "episode  167 score 245.25 timesteps: 107128\n",
            "episode  168 score 213.93 timesteps: 107688\n",
            "episode  169 score 242.44 timesteps: 108120\n",
            "episode  170 score 245.33 trailing 10 games avg 244.118 trailing 100 games avg 56.647 timesteps: 108650\n",
            "episode  171 score -1.47 timesteps: 109650\n",
            "episode  172 score 219.98 timesteps: 110353\n",
            "episode  173 score 211.46 timesteps: 111133\n",
            "episode  174 score 8.46 timesteps: 112133\n",
            "episode  175 score 106.44 timesteps: 113133\n",
            "episode  176 score 146.30 timesteps: 114133\n",
            "episode  177 score 91.42 timesteps: 115133\n",
            "episode  178 score 242.49 timesteps: 115580\n",
            "episode  179 score 250.25 timesteps: 116535\n",
            "episode  180 score 143.86 trailing 10 games avg 141.918 trailing 100 games avg 73.310 timesteps: 117535\n",
            "episode  181 score 243.90 timesteps: 117950\n",
            "episode  182 score 128.83 timesteps: 118950\n",
            "episode  183 score 275.25 timesteps: 119380\n",
            "episode  184 score 211.43 timesteps: 119893\n",
            "episode  185 score 200.63 timesteps: 120804\n",
            "episode  186 score 73.06 timesteps: 121804\n",
            "episode  187 score 38.93 timesteps: 122804\n",
            "episode  188 score 285.49 timesteps: 123403\n",
            "episode  189 score 279.10 timesteps: 123992\n",
            "episode  190 score 223.22 trailing 10 games avg 195.985 trailing 100 games avg 106.024 timesteps: 124846\n",
            "episode  191 score 226.42 timesteps: 125364\n",
            "episode  192 score 144.02 timesteps: 126364\n",
            "episode  193 score 263.68 timesteps: 126718\n",
            "episode  194 score 222.66 timesteps: 127354\n",
            "episode  195 score 283.20 timesteps: 127663\n",
            "episode  196 score 268.96 timesteps: 127940\n",
            "episode  197 score 264.96 timesteps: 128517\n",
            "episode  198 score 224.36 timesteps: 129170\n",
            "episode  199 score 258.79 timesteps: 129579\n",
            "episode  200 score 270.52 trailing 10 games avg 242.757 trailing 100 games avg 129.726 timesteps: 129884\n",
            "episode  201 score 212.63 timesteps: 130665\n",
            "episode  202 score 237.00 timesteps: 131383\n",
            "episode  203 score 240.96 timesteps: 131710\n",
            "episode  204 score 235.34 timesteps: 132635\n",
            "episode  205 score 46.93 timesteps: 133635\n",
            "episode  206 score 122.81 timesteps: 134635\n",
            "episode  207 score 162.77 timesteps: 135635\n",
            "episode  208 score 167.60 timesteps: 136635\n",
            "episode  209 score 152.84 timesteps: 137635\n",
            "episode  210 score 216.02 trailing 10 games avg 179.489 trailing 100 games avg 149.504 timesteps: 138037\n",
            "episode  211 score 136.64 timesteps: 139037\n",
            "episode  212 score 242.69 timesteps: 139367\n",
            "episode  213 score 251.48 timesteps: 139849\n",
            "episode  214 score 118.57 timesteps: 140849\n",
            "episode  215 score 213.06 timesteps: 141611\n",
            "episode  216 score 239.19 timesteps: 142221\n",
            "episode  217 score 256.96 timesteps: 142562\n",
            "episode  218 score 147.19 timesteps: 143562\n",
            "episode  219 score 245.81 timesteps: 143859\n",
            "episode  220 score 105.10 trailing 10 games avg 195.668 trailing 100 games avg 171.346 timesteps: 144859\n",
            "episode  221 score 219.59 timesteps: 145155\n",
            "episode  222 score 274.30 timesteps: 145504\n",
            "episode  223 score 241.10 timesteps: 145779\n",
            "episode  224 score 239.24 timesteps: 146418\n",
            "episode  225 score -180.02 timesteps: 146956\n",
            "episode  226 score 245.12 timesteps: 147376\n",
            "episode  227 score 259.51 timesteps: 147721\n",
            "episode  228 score 268.66 timesteps: 147985\n",
            "episode  229 score -1.41 timesteps: 148329\n",
            "episode  230 score 254.25 trailing 10 games avg 182.033 trailing 100 games avg 184.897 timesteps: 148656\n",
            "episode  231 score -10.29 timesteps: 148941\n",
            "episode  232 score 289.44 timesteps: 149231\n",
            "episode  233 score -24.57 timesteps: 149436\n",
            "episode  234 score 285.70 timesteps: 149692\n",
            "episode  235 score -9.32 timesteps: 149871\n",
            "episode  236 score -51.44 timesteps: 150871\n",
            "episode  237 score 157.92 timesteps: 151302\n",
            "episode  238 score 231.99 timesteps: 151567\n",
            "episode  239 score -60.43 timesteps: 152567\n",
            "episode  240 score 161.12 trailing 10 games avg 97.012 trailing 100 games avg 184.936 timesteps: 153404\n",
            "episode  241 score 229.07 timesteps: 153786\n",
            "episode  242 score -50.42 timesteps: 154786\n",
            "episode  243 score 195.31 timesteps: 155453\n",
            "episode  244 score 164.32 timesteps: 155780\n",
            "episode  245 score 302.50 timesteps: 156029\n",
            "episode  246 score -45.43 timesteps: 157029\n",
            "episode  247 score -12.87 timesteps: 157214\n",
            "episode  248 score -77.26 timesteps: 158214\n",
            "episode  249 score 253.68 timesteps: 158567\n",
            "episode  250 score 310.69 trailing 10 games avg 126.960 trailing 100 games avg 178.797 timesteps: 158795\n",
            "episode  251 score 257.51 timesteps: 159152\n",
            "episode  252 score 282.59 timesteps: 159413\n",
            "episode  253 score -12.30 timesteps: 160413\n",
            "episode  254 score 98.40 timesteps: 161413\n",
            "episode  255 score -37.64 timesteps: 162413\n",
            "episode  256 score 278.00 timesteps: 162841\n",
            "episode  257 score -67.96 timesteps: 163841\n",
            "episode  258 score 241.56 timesteps: 164210\n",
            "episode  259 score -98.16 timesteps: 165210\n",
            "episode  260 score 182.03 trailing 10 games avg 112.403 trailing 100 games avg 171.834 timesteps: 165990\n",
            "episode  261 score 278.05 timesteps: 166258\n",
            "episode  262 score 221.24 timesteps: 166584\n",
            "episode  263 score 246.37 timesteps: 166879\n",
            "episode  264 score 249.91 timesteps: 167163\n",
            "episode  265 score 239.64 timesteps: 167647\n",
            "episode  266 score -25.17 timesteps: 168647\n",
            "episode  267 score -75.33 timesteps: 169647\n",
            "episode  268 score 230.83 timesteps: 169945\n",
            "episode  269 score 255.87 timesteps: 170144\n",
            "episode  270 score 137.12 trailing 10 games avg 175.852 trailing 100 games avg 165.008 timesteps: 171144\n",
            "episode  271 score -60.31 timesteps: 172144\n",
            "episode  272 score -152.09 timesteps: 173104\n",
            "episode  273 score -53.77 timesteps: 174104\n",
            "episode  274 score -48.91 timesteps: 175104\n",
            "episode  275 score -76.43 timesteps: 176104\n",
            "episode  276 score 252.33 timesteps: 176391\n",
            "episode  277 score -56.11 timesteps: 177391\n",
            "episode  278 score 232.62 timesteps: 177727\n",
            "episode  279 score 258.51 timesteps: 178115\n",
            "episode  280 score 247.31 trailing 10 games avg 54.312 trailing 100 games avg 156.247 timesteps: 178398\n",
            "episode  281 score -95.28 timesteps: 179398\n",
            "episode  282 score -294.26 timesteps: 179634\n",
            "episode  283 score -36.97 timesteps: 180634\n",
            "episode  284 score 233.81 timesteps: 180883\n",
            "episode  285 score -63.22 timesteps: 181883\n",
            "episode  286 score 234.83 timesteps: 182217\n",
            "episode  287 score 239.63 timesteps: 182716\n",
            "episode  288 score 288.81 timesteps: 183005\n",
            "episode  289 score 251.76 timesteps: 183196\n",
            "episode  290 score 48.09 trailing 10 games avg 80.719 trailing 100 games avg 144.721 timesteps: 184196\n",
            "episode  291 score 217.64 timesteps: 184502\n",
            "episode  292 score 234.91 timesteps: 184702\n",
            "episode  293 score 219.28 timesteps: 185041\n",
            "episode  294 score -79.21 timesteps: 186041\n",
            "episode  295 score 221.95 timesteps: 186579\n",
            "episode  296 score -103.03 timesteps: 187579\n",
            "episode  297 score -99.55 timesteps: 188579\n",
            "episode  298 score 287.59 timesteps: 188860\n",
            "episode  299 score -25.07 timesteps: 189860\n",
            "episode  300 score 291.62 trailing 10 games avg 116.615 trailing 100 games avg 132.106 timesteps: 190251\n",
            "episode  301 score 283.18 timesteps: 190608\n",
            "episode  302 score -87.25 timesteps: 191608\n",
            "episode  303 score 277.81 timesteps: 191905\n",
            "episode  304 score 247.32 timesteps: 192141\n",
            "episode  305 score 239.76 timesteps: 192403\n",
            "episode  306 score -107.94 timesteps: 193403\n",
            "episode  307 score -82.61 timesteps: 194403\n",
            "episode  308 score -106.62 timesteps: 194730\n",
            "episode  309 score 210.76 timesteps: 195059\n",
            "episode  310 score 278.03 trailing 10 games avg 115.243 trailing 100 games avg 125.682 timesteps: 195345\n",
            "episode  311 score 218.40 timesteps: 195604\n",
            "episode  312 score 275.22 timesteps: 195887\n",
            "episode  313 score -103.65 timesteps: 196251\n",
            "episode  314 score 264.68 timesteps: 196527\n",
            "episode  315 score 165.40 timesteps: 197245\n",
            "episode  316 score 281.45 timesteps: 197533\n",
            "episode  317 score 270.70 timesteps: 198088\n",
            "episode  318 score -42.24 timesteps: 199088\n",
            "episode  319 score -172.93 timesteps: 199257\n",
            "episode  320 score 239.27 trailing 10 games avg 139.629 trailing 100 games avg 120.078 timesteps: 199477\n",
            "episode  321 score -181.31 timesteps: 200052\n",
            "episode  322 score -96.80 timesteps: 200445\n",
            "episode  323 score 299.18 timesteps: 200695\n",
            "episode  324 score 292.17 timesteps: 200993\n",
            "episode  325 score 88.66 timesteps: 201993\n",
            "episode  326 score -128.46 timesteps: 202399\n",
            "episode  327 score -99.16 timesteps: 202800\n",
            "episode  328 score 248.80 timesteps: 203321\n",
            "episode  329 score 280.71 timesteps: 203559\n",
            "episode  330 score -181.77 trailing 10 games avg 52.201 trailing 100 games avg 107.095 timesteps: 204211\n",
            "episode  331 score 268.38 timesteps: 204525\n",
            "episode  332 score 179.70 timesteps: 205062\n",
            "episode  333 score -126.11 timesteps: 206062\n",
            "episode  334 score 260.39 timesteps: 206362\n",
            "episode  335 score 296.69 timesteps: 206616\n",
            "episode  336 score -61.81 timesteps: 207616\n",
            "episode  337 score 244.13 timesteps: 207918\n",
            "episode  338 score 272.77 timesteps: 208171\n",
            "episode  339 score 31.15 timesteps: 208307\n",
            "episode  340 score -139.56 trailing 10 games avg 122.575 trailing 100 games avg 109.651 timesteps: 209307\n",
            "episode  341 score 250.31 timesteps: 209518\n",
            "episode  342 score 265.84 timesteps: 209911\n",
            "episode  343 score 253.17 timesteps: 210161\n",
            "episode  344 score 260.61 timesteps: 210652\n",
            "episode  345 score 44.58 timesteps: 211652\n",
            "episode  346 score 259.16 timesteps: 211907\n",
            "episode  347 score -214.54 timesteps: 212824\n",
            "episode  348 score -98.10 timesteps: 213824\n",
            "episode  349 score 263.96 timesteps: 214010\n",
            "episode  350 score 57.75 trailing 10 games avg 134.274 trailing 100 games avg 110.382 timesteps: 215010\n",
            "episode  351 score -58.43 timesteps: 216010\n",
            "episode  352 score 253.09 timesteps: 216220\n",
            "episode  353 score 195.73 timesteps: 217146\n",
            "episode  354 score 275.21 timesteps: 217381\n",
            "episode  355 score 243.29 timesteps: 217825\n",
            "episode  356 score 242.58 timesteps: 218110\n",
            "episode  357 score 241.23 timesteps: 218364\n",
            "episode  358 score 259.54 timesteps: 218740\n",
            "episode  359 score 275.39 timesteps: 218902\n",
            "episode  360 score 277.15 trailing 10 games avg 220.478 trailing 100 games avg 121.190 timesteps: 219089\n",
            "episode  361 score 197.71 timesteps: 219781\n",
            "episode  362 score 269.56 timesteps: 220176\n",
            "episode  363 score 264.96 timesteps: 220739\n",
            "episode  364 score 130.27 timesteps: 221739\n",
            "episode  365 score 244.52 timesteps: 222083\n",
            "episode  366 score 245.39 timesteps: 222669\n",
            "episode  367 score 294.07 timesteps: 222963\n",
            "episode  368 score 272.08 timesteps: 223180\n",
            "episode  369 score 256.67 timesteps: 223357\n",
            "episode  370 score 260.31 trailing 10 games avg 243.553 trailing 100 games avg 127.960 timesteps: 223615\n",
            "episode  371 score 266.27 timesteps: 223825\n",
            "episode  372 score 93.27 timesteps: 224825\n",
            "episode  373 score 279.93 timesteps: 225006\n",
            "episode  374 score 260.54 timesteps: 225297\n",
            "episode  375 score 270.49 timesteps: 225517\n",
            "episode  376 score 233.53 timesteps: 225781\n",
            "episode  377 score 293.54 timesteps: 225987\n",
            "episode  378 score 221.44 timesteps: 226828\n",
            "episode  379 score 261.04 timesteps: 227043\n",
            "episode  380 score 280.18 trailing 10 games avg 246.023 trailing 100 games avg 147.131 timesteps: 227285\n",
            "episode  381 score 276.12 timesteps: 227528\n",
            "episode  382 score 244.47 timesteps: 227920\n",
            "episode  383 score 283.98 timesteps: 228141\n",
            "episode  384 score 262.72 timesteps: 228660\n",
            "episode  385 score 272.75 timesteps: 229178\n",
            "episode  386 score 268.46 timesteps: 229368\n",
            "episode  387 score 277.66 timesteps: 229546\n",
            "episode  388 score 276.38 timesteps: 229752\n",
            "episode  389 score 229.07 timesteps: 230454\n",
            "episode  390 score 265.75 trailing 10 games avg 265.736 trailing 100 games avg 165.633 timesteps: 230812\n",
            "episode  391 score 253.04 timesteps: 231320\n",
            "episode  392 score 261.06 timesteps: 231486\n",
            "episode  393 score 174.47 timesteps: 232258\n",
            "episode  394 score 303.91 timesteps: 232499\n",
            "episode  395 score 267.57 timesteps: 232729\n",
            "episode  396 score -29.57 timesteps: 233729\n",
            "episode  397 score 231.38 timesteps: 233936\n",
            "episode  398 score 289.56 timesteps: 234232\n",
            "episode  399 score 237.29 timesteps: 234407\n",
            "episode  400 score 266.84 trailing 10 games avg 225.557 trailing 100 games avg 176.527 timesteps: 234575\n",
            "episode  401 score 297.85 timesteps: 234820\n",
            "episode  402 score 280.28 timesteps: 235230\n",
            "episode  403 score 263.33 timesteps: 235411\n",
            "episode  404 score 280.69 timesteps: 235661\n",
            "episode  405 score 307.77 timesteps: 235890\n",
            "episode  406 score 233.68 timesteps: 236097\n",
            "episode  407 score 249.57 timesteps: 236293\n",
            "episode  408 score -38.25 timesteps: 236434\n",
            "episode  409 score 291.44 timesteps: 236735\n",
            "episode  410 score 244.15 trailing 10 games avg 241.049 trailing 100 games avg 189.107 timesteps: 236959\n",
            "episode  411 score 269.90 timesteps: 237189\n",
            "episode  412 score 295.13 timesteps: 237397\n",
            "episode  413 score 275.98 timesteps: 237719\n",
            "episode  414 score 231.17 timesteps: 237907\n",
            "episode  415 score 273.50 timesteps: 238287\n",
            "episode  416 score 249.20 timesteps: 238628\n",
            "episode  417 score 279.26 timesteps: 238866\n",
            "episode  418 score 314.18 timesteps: 239066\n",
            "episode  419 score 286.68 timesteps: 239240\n",
            "episode  420 score 301.97 trailing 10 games avg 277.696 trailing 100 games avg 202.914 timesteps: 239576\n",
            "episode  421 score 246.94 timesteps: 239764\n",
            "episode  422 score 265.79 timesteps: 239967\n",
            "####SOLVED!!####\n",
            "\n",
            "Episode Reward: 290.3702122625633\n",
            "Episode Reward: 284.2367776898686\n",
            "Episode Reward: 264.74984768405994\n",
            "Episode Reward: 245.3880596342788\n",
            "Episode Reward: 269.3106314983925\n",
            "Episode Reward: 258.04864883476847\n",
            "Episode Reward: 284.18400433993065\n",
            "Episode Reward: 270.5608364085198\n",
            "Episode Reward: 261.0650962500996\n",
            "Episode Reward: 269.2244368893939\n",
            "Episode Reward: 253.47071383713427\n",
            "Episode Reward: 265.68917385415165\n",
            "Episode Reward: 307.26486975421847\n",
            "Episode Reward: 297.2997552316464\n",
            "Episode Reward: 259.0275686160592\n",
            "Episode Reward: 258.1857613171859\n",
            "Episode Reward: 278.7532650567572\n",
            "Episode Reward: 294.54307276425516\n",
            "Episode Reward: 271.563606072742\n",
            "Episode Reward: 274.45092917188225\n",
            "Episode Reward: 266.2816472669502\n",
            "Episode Reward: 273.5695821932021\n",
            "Episode Reward: 293.2650601448941\n",
            "Episode Reward: 247.0211164043244\n",
            "Episode Reward: 264.93942454567116\n",
            "Episode Reward: 273.6052680605272\n",
            "Episode Reward: 280.77039375810705\n",
            "Episode Reward: 293.64396311612677\n",
            "Episode Reward: 253.34403752518207\n",
            "Episode Reward: 297.3608087640896\n",
            "Episode Reward: 246.67426693972476\n",
            "Episode Reward: 278.773409203627\n",
            "Episode Reward: 273.9601633890016\n",
            "Episode Reward: 253.38118473392208\n",
            "Episode Reward: 273.7540500613958\n",
            "Episode Reward: 271.8629255523538\n",
            "Episode Reward: 299.02012770274655\n",
            "Episode Reward: 292.4656308058708\n",
            "Episode Reward: 232.5361577578801\n",
            "Episode Reward: 285.33257795653185\n",
            "Episode Reward: 285.78638538656\n",
            "Episode Reward: 285.6943738348572\n",
            "Episode Reward: 251.96510277550874\n",
            "Episode Reward: 269.6182609119587\n",
            "Episode Reward: 262.3821991066218\n",
            "Episode Reward: 283.96134763449425\n",
            "Episode Reward: 275.3897213814362\n",
            "Episode Reward: 268.5059751911149\n",
            "Episode Reward: 123.463322786358\n",
            "Episode Reward: 290.52441787165355\n",
            "\n",
            "270.12480339861196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OssoiIFdbK92"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}